# GENESPACE analysis

This github page will explain how to replicate the GENESPACE analysis conducted on the diploid Cardamine amara, and two other diploid Cardamine hirsuta assemblies.

+ [Prerequisites](#prerequisites)
  - [Tool Version and Links](#tool-version-and-links)
  - [Tool Installation](#tool-installation)
  - [Data Acquisition](#data-acquisition)
 
    - [Cardamine amara (Haplome 1)](#cardamine-amara-haplome-1)
    - [Cardamine amara (Haplome 2)](#cardamine-amara-haplome-2)
    - [Cardamine hirsuta (Sanger)](#cardamine-hirsuta-sanger)
    - [Cardamine hirsuta (Max Planck)](#cardamine-hirsuta-max-planck)
    
+ [Analysis](#the-analysis)
  


# Prerequisites

## Tool version and links

## Tool Installation
Yaml files

## Data Acquisition

To run genespace we need protein fastas for Cardamine amara (Haplome 1), Cardamine amara (Haplome 2), Cardamine hirsuta (Sanger), Cardamine hirsuta (Max Planck).

### Cardamine amara (Haplome 1)
Haplome 1 for Cardamine amara was created by R.

```bash
mkdir -p ~/Cardamine_Annotation_Haplomes/Haplome1/Input_Seqs
cp /path/to/haplome1/assembly ~/Cardamine_Annotation_Haplomes/Haplome1/Input_Seqs/haplome1.fa
```

### Cardamine amara (Haplome 2)
Haplome 2 for Cardamine amara was created by R.

```bash
mkdir -p ~/Cardamine_Annotation_Haplomes/Haplome2/Input_Seqs
cp /path/to/haplome2/assembly ~/Cardamine_Annotation_Haplomes/Haplome2/Input_Seqs/haplome2.fa
```

### Cardamine hirsuta (Sanger)
The Cardamine hirsuta assembly produced by Sanger is hosted on NCBI. To download the assembly run the code below:

```bash
#!/bin/bash

#SBATCH --job-name=downloading_cardamine_hirsuta
#SBATCH --partition=shortq
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=30
#SBATCH --mem=24g
#SBATCH --time=01:00:00
#SBATCH --output=/path/to/Output/and/Error/dir/OnE/%x.out
#SBATCH --error=/path/to/Output/and/Error/dir/OnE/%x.err

#initialise conda
source ~/anaconda3/etc/profile.d/conda.sh

# activate conda env with ncbi datasets
conda activate ncbi_datasets_env

# define directory for output
OUTPUTDIR=~/Cardamine_Annotation_Haplomes/Shared_Input_Data/NCBI_Data/Sanger_Hirsuta

# create output directory if it does not exist already
mkdir -p $OUTPUTDIR

# move into output directory
cd $OUTPUTDIR

# download cardamine hirsuta dataset
datasets download genome accession GCA_964212585.1 \
	--include genome \
	--filename Sanger_Hirsuta_dataset.zip

# unzip
unzip Sanger_Hirsuta_dataset.zip

# deactivate conda env
conda deactivate

```

### Cardamine hirsuta (Max Planck)
The Cardamine hirsuta assembly generated by MaxPlanck can be found [here](https://chi.mpipz.mpg.de/download/annotations)

```bash
# define directory for output
OUTPUTDIR=~/Cardamine_Annotation_Haplomes/Shared_Input_Data/MaxPlanck_Hirsuta

# create output directory if it does not exist already
mkdir -p $OUTPUTDIR

# move into output directory
cd $OUTPUTDIR

# download gff to current dir with
wget -O Chirsuta.gff https://chi.mpipz.mpg.de/download/annotations/carhr38.gff

# download peptides to current dir
wget -O Chirsuta.pep.fa https://chi.mpipz.mpg.de/download/annotations/carhr38.aa.fa

# download fasta
wget -O Chirsuta.nucl.fa https://chi.mpipz.mpg.de/download/sequences/chi_v1.fa
```
# The analysis

## <ins>RepeatMasker/RepeatModeller<ins>

```bash
#!/bin/bash

#SBATCH --job-name=running_repeatmodellermasker_haplomes_v1
#SBATCH --partition=shortq
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=48
#SBATCH --mem=40g
#SBATCH --time=12:00:00
#SBATCH --output=/path/to/output/and/error/directory/%x.out
#SBATCH --error=/path/to/output/and/error/directory/%x.err

#initialise conda
source ~/anaconda3/etc/profile.d/conda.sh

# activate Rmodel/mask env
conda activate RModeller-Masker

## ENTIRE HAPLOME 1
# define directory for output and where input sequences can be found
INPUTSEQ=~/Cardamine_Annotation_Haplomes/Haplome1/Input_Seqs/haplome1.fa
OUTPUTDIR=~/Cardamine_Annotation_Haplomes/Haplome1/Output/RMasker

# create output directory if it does not exist already
mkdir -p $OUTPUTDIR

# move into output directory
cd $OUTPUTDIR

# mask first haplome
BuildDatabase -name haplome1_db $INPUTSEQ
RepeatModeler -database haplome1_db -threads 48 -LTRStruct
RepeatMasker -pa 48 -dir $OUTPUTDIR -lib haplome1_db-families.fa -xsmall $INPUTSEQ

# chnage name to soft mask
cp haplome1.fa.masked haplome1.softmasked.fa

## ENTIRE HAPLOME 2
# define directory for output and where input sequences can be found
INPUTSEQ=~/Cardamine_Annotation_Haplomes/Haplome2/Input_Seqs/haplome2.fa
OUTPUTDIR=~/Cardamine_Annotation_Haplomes/Haplome2/Output/RMasker

# create output directory if it does not exist already
mkdir -p $OUTPUTDIR

# move into output directory
cd $OUTPUTDIR

# mask first haplome
BuildDatabase -name haplome2_db $INPUTSEQ
RepeatModeler -database haplome2_db -threads 48 -LTRStruct
RepeatMasker -pa 48 -dir $OUTPUTDIR -lib haplome2_db-families.fa -xsmall $INPUTSEQ

# chnage name to soft mask
cp haplome2.fa.masked haplome2.softmasked.fa

# deactivate conda env
conda deactivate

# get job id
echo "The Job ID for this job is: $SLURM_JOB_ID"

# script should take 12 hours at most with 40 cpus, and you will need at least 30 Gb of space
```


