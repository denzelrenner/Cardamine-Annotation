# GENESPACE analysis

This github page will explain how to replicate the GENESPACE analysis conducted on the diploid Cardamine amara, and two other diploid Cardamine hirsuta assemblies.

+ [Prerequisites](#prerequisites)
  - [Tool Version and Links](#tool-version-and-links)
  - [Tool Installation](#tool-installation)
  - [Data Acquisition](#data-acquisition)
 
    - [Cardamine amara (Haplome 1)](#cardamine-amara-haplome-1)
    - [Cardamine amara (Haplome 2)](#cardamine-amara-haplome-2)
    - [Cardamine hirsuta (Sanger)](#cardamine-hirsuta-sanger)
    - [Cardamine hirsuta (Max Planck)](#cardamine-hirsuta-max-planck)
    
+ [Analysis](#the-analysis)
  


# Prerequisites

## <ins>Tool version and links<ins>

## <ins>Tool Installation<ins>
Yaml files

## <ins>Data Acquisition<ins>

To run genespace we need protein fastas for Cardamine amara (Haplome 1), Cardamine amara (Haplome 2), Cardamine hirsuta (Sanger), Cardamine hirsuta (Max Planck).

### <ins>Cardamine amara (Haplome 1)<ins>
Haplome 1 for Cardamine amara was created by R.

```bash
mkdir -p ~/Cardamine_Annotation_Haplomes/Haplome1/Input_Seqs
cp /path/to/haplome1/assembly ~/Cardamine_Annotation_Haplomes/Haplome1/Input_Seqs/haplome1.fa
```

### <ins>Cardamine amara (Haplome 2)<ins>
Haplome 2 for Cardamine amara was created by R.

```bash
mkdir -p ~/Cardamine_Annotation_Haplomes/Haplome2/Input_Seqs
cp /path/to/haplome2/assembly ~/Cardamine_Annotation_Haplomes/Haplome2/Input_Seqs/haplome2.fa
```

### <ins>Cardamine hirsuta (Sanger)<ins>
The Cardamine hirsuta assembly produced by Sanger is hosted on NCBI. To download the assembly run the code below:

```bash
#!/bin/bash

#SBATCH --job-name=downloading_cardamine_hirsuta
#SBATCH --partition=shortq
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=30
#SBATCH --mem=24g
#SBATCH --time=01:00:00
#SBATCH --output=/path/to/Output/and/Error/dir/OnE/%x.out
#SBATCH --error=/path/to/Output/and/Error/dir/OnE/%x.err

#initialise conda
source ~/anaconda3/etc/profile.d/conda.sh

# activate conda env with ncbi datasets
conda activate ncbi_datasets_env

# define directory for output
OUTPUTDIR=~/Cardamine_Annotation_Haplomes/Shared_Input_Data/NCBI_Data/Sanger_Hirsuta

# create output directory if it does not exist already
mkdir -p $OUTPUTDIR

# move into output directory
cd $OUTPUTDIR

# download cardamine hirsuta dataset
datasets download genome accession GCA_964212585.1 \
	--include genome \
	--filename Sanger_Hirsuta_dataset.zip

# unzip
unzip Sanger_Hirsuta_dataset.zip

# deactivate conda env
conda deactivate

```

### <ins>Cardamine hirsuta (Max Planck)<ins>
The Cardamine hirsuta assembly and annotations generated by MaxPlanck can be found [here](https://chi.mpipz.mpg.de/download/annotations)

```bash
# define directory for output
OUTPUTDIR=~/Cardamine_Annotation_Haplomes/Shared_Input_Data/MaxPlanck_Hirsuta

# create output directory if it does not exist already
mkdir -p $OUTPUTDIR

# move into output directory
cd $OUTPUTDIR

# download gff to current dir with
wget -O Chirsuta.gff https://chi.mpipz.mpg.de/download/annotations/carhr38.gff

# download peptides to current dir
wget -O Chirsuta.pep.fa https://chi.mpipz.mpg.de/download/annotations/carhr38.aa.fa

# download fasta
wget -O Chirsuta.nucl.fa https://chi.mpipz.mpg.de/download/sequences/chi_v1.fa
```
# The Analysis

## <ins>RepeatMasker/RepeatModeller<ins>

```bash
#!/bin/bash

#SBATCH --job-name=running_repeatmodellermasker_all_assemblies
#SBATCH --partition=hmemq
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=48
#SBATCH --mem=100g
#SBATCH --time=60:00:00
#SBATCH --output=/path/to/output/and/error/directory/%x.out
#SBATCH --error=/path/to/output/and/error/directory/%x.err

#initialise conda
source ~/anaconda3/etc/profile.d/conda.sh

# activate Rmodel/mask env
conda activate RModeller-Masker

## ENTIRE HAPLOME 1
# define directory for output and where input sequences can be found
INPUTSEQ=~/Cardamine_Annotation_Haplomes/Haplome1/Input_Seqs/haplome1.fa
OUTPUTDIR=~/Cardamine_Annotation_Haplomes/Haplome1/Output/RMasker

# create output directory if it does not exist already
mkdir -p $OUTPUTDIR

# move into output directory
cd $OUTPUTDIR

# mask first haplome
BuildDatabase -name haplome1_db $INPUTSEQ
RepeatModeler -database haplome1_db -threads 48 -LTRStruct
RepeatMasker -pa 48 -dir $OUTPUTDIR -lib haplome1_db-families.fa -xsmall $INPUTSEQ

# chnage name to soft mask
cp haplome1.fa.masked haplome1.softmasked.fa

## ENTIRE HAPLOME 2
# define directory for output and where input sequences can be found
INPUTSEQ=~/Cardamine_Annotation_Haplomes/Haplome2/Input_Seqs/haplome2.fa
OUTPUTDIR=~/Cardamine_Annotation_Haplomes/Haplome2/Output/RMasker

# create output directory if it does not exist already
mkdir -p $OUTPUTDIR

# move into output directory
cd $OUTPUTDIR

# mask first haplome
BuildDatabase -name haplome2_db $INPUTSEQ
RepeatModeler -database haplome2_db -threads 48 -LTRStruct
RepeatMasker -pa 48 -dir $OUTPUTDIR -lib haplome2_db-families.fa -xsmall $INPUTSEQ

# chnage name to soft mask
cp haplome2.fa.masked haplome2.softmasked.fa

# sanger hirsuta
# define directory for output and where input sequences can be found
INPUTSEQ=~/Cardamine_Annotation_Haplomes/Shared_Input_Data/NCBI_Data/Sanger_Hirsuta/ncbi_dataset/data/GCA_964212585.1/GCA_964212585.1_ddCarHirs1.hap1.1_genomic.fna
OUTPUTDIR=~/Cardamine_Annotation_Haplomes/Shared_Output_Data/Sanger_Hirusta/RMasker

# create output directory if it does not exist already
mkdir -p $OUTPUTDIR

# move into output directory
cd $OUTPUTDIR

# mask first haplome
BuildDatabase -name sanger_haplome1_db $INPUTSEQ
RepeatModeler -database sanger_haplome1_db -threads 48 -LTRStruct
RepeatMasker -pa 48 -dir $OUTPUTDIR -lib sanger_haplome1_db-families.fa -xsmall $INPUTSEQ

# chnage name to soft mask
cp GCA_964212585.1_ddCarHirs1.hap1.1_genomic.fna.masked sanger_hirsuta_haplome1.softmasked.fa

# deactivate conda env
conda deactivate

# get job id
echo "The Job ID for this job is: $SLURM_JOB_ID"
```

## GENESPACE

```bash
#!/bin/bash

#SBATCH --job-name=genespace
#SBATCH --partition=shortq
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=40
#SBATCH --mem=50g
#SBATCH --time=12:00:00
#SBATCH --output=/path/to/output/and/error/directory/%x.out
#SBATCH --error=/path/to/output/and/error/directory/%x.err

# conda init, init?
source ~/anaconda3/etc/profile.d/conda.sh

# load conda env
conda activate genespace4

# make required directories for genespace
#OutputDir=~/Cardamine_Annotation_Haplomes/Shared_Output_Data/Genespace_NoPairwise
#GenespaceInputDataDir=~/Cardamine_Annotation_Haplomes/Shared_Output_Data/Genespace_NoPairwise/InputData
#ScriptsDir=~/Cardamine_Annotation_Haplomes/Shared_Output_Data/Genespace_NoPairwise/Scripts

OutputDir=~/Cardamine_Annotation_Haplomes/Shared_Output_Data/Genespace_Results/Genespace_Sanger_Amara
GenespaceInputDataDir=~/Cardamine_Annotation_Haplomes/Shared_Output_Data/Genespace_Results/Genespace_Sanger_Amara/InputData
ScriptsDir=~/Cardamine_Annotation_Haplomes/Shared_Output_Data/Genespace_Results/Genespace_Sanger_Amara/Scripts
Haplome1DataDir=~/Cardamine_Annotation_Haplomes/Haplome1/Output/Braker
Haplome2DataDir=~/Cardamine_Annotation_Haplomes/Haplome2/Output/Braker
PlanckHirsutaDataDir=~/Cardamine_Annotation_Haplomes/Shared_Input_Data/MaxPlanck_Hirsuta
SangerHirsutaDataDir=~/Cardamine_Annotation_Haplomes/Shared_Output_Data/Sanger_Hirsuta/Braker

# make output dir
mkdir -p $OutputDir

# move into main output dir
# make sub directories
cd $OutputDir

mkdir -p InputData

# get input files
cd $GenespaceInputDataDir

# copy haplome1 and 2 proteins
cp $Haplome1DataDir"/braker.aa" haplome1_braker.fa
cp $Haplome2DataDir"/braker.aa" haplome2_braker.fa
#cp $PlanckHirsutaDataDir"/Chirsuta.pep.fa" cardamine_hirsuta_planck.fa
cp $SangerHirsutaDataDir"/braker.aa" cardamine_hirsuta_sanger.fa

# copy gtfs to dir
cp $Haplome1DataDir"/braker.gtf" haplome1_braker.gtf
cp $Haplome2DataDir"/braker.gtf" haplome2_braker.gtf
#cp $GlacuaDataDir1"/braker.gtf" cardamine_glacua_softmask_braker.gtf
#cp $GlacuaDataDir2"/braker.gtf" cardamine_glacua_primaryassem_braker.gtf
#cp $CEnshiensisDataDir"/Censhiensis.gff" cardamine_enshiensis.gff
#cp $PlanckHirsutaDataDir"/Chirsuta.gff" cardamine_hirsuta_planck.gff
cp $SangerHirsutaDataDir"/braker.gtf" cardamine_hirsuta_sanger.gtf

# make bed file for max planck hirsuta
# get all alternative transcripts, so we dont grep gene which just gives us the gene name
#grep -P '\tmRNA\t' cardamine_hirsuta_planck.gff | grep 'Chr' > filtered_cardamine_hirsuta_planck.gff
#python3 ~/Cardamine_Annotation_Haplomes/Scripts/Python_Scripts/convert_gff_to_bed.py --gff filtered_cardamine_hirsuta_planck.gff \
#	-o cardamine_hirsuta_planck.bed --gene_index 0

# create debris scaffold for hirsuta assembly
python3 ~/Cardamine_Annotation_Haplomes/Scripts/Python_Scripts/create_debris_scaffolds.py \
	--nucl ~/Cardamine_Annotation_Haplomes/Shared_Input_Data/NCBI_Data/Sanger_Hirsuta/ncbi_dataset/data/GCA_964212585.1/GCA_964212585.1_ddCarHirs1.hap1.1_genomic.fna \
	--gff cardamine_hirsuta_sanger.gtf --chr 8 -o cardamine_hirsuta_sanger.gff -d ./ --sep _ --chr_prefix RL_ --debris RL_9 --rename

# convert to bed
grep -P '\ttranscript\t' haplome1_braker.gtf | awk '{print $1,$4,$5,$9}' > haplome1_braker.bed
grep -P '\ttranscript\t' haplome2_braker.gtf | awk '{print $1,$4,$5,$9}' > haplome2_braker.bed
grep -P '\ttranscript\t' cardamine_hirsuta_sanger.gff | awk '{print $1,$4,$5,$9}' > cardamine_hirsuta_sanger.bed

# remove gtf files
rm haplome1_braker.gtf
rm haplome2_braker.gtf
#rm cardamine_glacua_softmask_braker.gtf
#rm cardamine_glacua_primaryassem_braker.gtf
rm cardamine_hirsuta_sanger.gtf
rm cardamine_hirsuta_sanger.gff
#rm cardamine_enshiensis.gff
#rm filtered_cardamine_enshiensis.gff
#rm filtered_cardamine_hirsuta_planck.gff
#rm cardamine_hirsuta_planck.gff

# run python script to sort all this input data and create genespace scripts
python3 ~/Cardamine_Annotation_Haplomes/Scripts/Python_Scripts/create_riparian_plots.py -od $OutputDir \
	-id $GenespaceInputDataDir \
	--reference cardamine_hirsuta_sanger

# move into scripts dir
cd $ScriptsDir

# run script
for file in *; do
    echo $file
    if [ -f $file ]; then
        echo "Running ${file} .."
	R -f $file
    fi
done

# deactivate env
conda deactivate

# echo job id
echo "The Job ID for this job is: $SLURM_JOB_ID"

```


